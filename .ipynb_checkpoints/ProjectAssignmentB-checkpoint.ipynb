{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02806 - Social Data Analysis and Viszalization -  Project Assignment B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Package and data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import json\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataByYear(year):\n",
    "    '''\n",
    "    A function to combine the data records of a year to a dataframe.\n",
    "    Due to different headers in the individual files (spaces, upper and\n",
    "    lower case letters, etc.), these are normalized.\n",
    "    Output is a Pandas Dataframe for the requested year.\n",
    "    '''\n",
    "    name=\"\"\n",
    "    df=[]\n",
    "    \n",
    "    for k in range(1,13):\n",
    "        if k<10:\n",
    "            name = str(year)+\"0\"+str(k)\n",
    "        else:\n",
    "            name = str(year)+\"\"+str(k)\n",
    "        name +=\"-citibike-tripdata.csv\"\n",
    "        \n",
    "        df.append(pd.read_csv('data/'+name))\n",
    "        #set columns to lower case\n",
    "        df[k-1].columns = map(str.lower, df[k-1].columns)\n",
    "        #Filter data \n",
    "        df[k-1].columns = [x.replace(\" \",\"\") for x in list(df[k-1].columns)]\n",
    "        df[k-1][\"starttime\"] = pd.to_datetime(df[k-1][\"starttime\"], infer_datetime_format=True)\n",
    "        df[k-1][\"stoptime\"] = pd.to_datetime(df[k-1][\"stoptime\"], infer_datetime_format=True)\n",
    "        \n",
    "    return pd.concat(df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset of years 2016-2018\n",
    "df16=getDataByYear(2016)\n",
    "df17=getDataByYear(2017)\n",
    "df18=getDataByYear(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all trip data\n",
    "frames = [df16,df17,df18]\n",
    "df_trips=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data from json url. Delete unnecessary columns\n",
    "with urllib.request.urlopen(\"https://feeds.citibikenyc.com/stations/stations.json\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    del data['executionTime']\n",
    "    data = data['stationBeanList']\n",
    "\n",
    "#Add data from json objects to list. Only include necessary fields\n",
    "stations = []\n",
    "for station in data:\n",
    "    stations.append([station['id'], station['stationName'], station['latitude'], station['longitude'], station['totalDocks']])\n",
    "\n",
    "#make dataframe of station info from list derived from json object\n",
    "df_stations = pd.DataFrame(stations, columns=[\"id\", \"stationname\", \"lat\", \"long\", \"capacity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Motivation.\n",
    "What is your dataset?\n",
    "\n",
    "Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "What was your goal for the end user's experience?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic stats. \n",
    "\n",
    "Let's understand the dataset better\n",
    "\n",
    "\n",
    "Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "\n",
    "Write a short section that discusses the dataset stats, containing key points/plots from your exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Analysis\n",
    "\n",
    "Describe your data analysis and explain what you've learned about the dataset.\n",
    "\n",
    "If relevant, talk about your machine-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete unnecessary columns and change to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand = df_trips.copy()\n",
    "del_cols = ['tripduration', 'startstationname', 'startstationlatitude', 'startstationlongitude', 'endstationname',\n",
    "       'endstationlatitude', 'endstationlongitude', 'usertype', 'birthyear', 'gender', 'bikeid']\n",
    "df_demand = df_demand.drop(del_cols, axis=1)\n",
    "df_demand['startday'] = df_demand['starttime'].dt.dayofweek\n",
    "df_demand['stopday'] = df_demand['stoptime'].dt.dayofweek\n",
    "df_demand['starttime'] = df_demand['starttime'].dt.hour\n",
    "df_demand['stoptime'] = df_demand['stoptime'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find stations that have both incoming and outgoing bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_demand = df_demand[['startstationid']].drop_duplicates()\n",
    "df3_demand = df_demand[['endstationid']].drop_duplicates()\n",
    "df4_demand = pd.merge(df2_demand,df3_demand, how='inner', left_on='startstationid', right_on='endstationid')\n",
    "df4_demand = df4_demand[['startstationid']].drop_duplicates().dropna()\n",
    "df4_demand['startstationid'] = df4_demand['startstationid'].apply(lambda x : int(x))\n",
    "df4_demand.columns = ['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find stations that are in both dataset, and merge these. Save data to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_common_stations = pd.merge(df1_stations, df4_demand, how='inner', on=['id'])\n",
    "df_demand_common_stations.to_csv('common_stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count bikes in and out for weekdays and hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_start = df_demand[['startstationid', 'startday','starttime']]\n",
    "df_demand_stop = df_demand[['endstationid', 'stopday', 'stoptime']]\n",
    "df_demand_start= pd.DataFrame({'startcount' : df_demand_start.groupby(['startstationid', 'startday', 'starttime']).size()}).reset_index()\n",
    "df_demand_stop = pd.DataFrame({'stopcount' : df_demand_stop.groupby(['endstationid', 'stopday', 'stoptime']).size()}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge in and out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_count = pd.merge(df_demand_start, df_demand_stop, left_on=['startstationid', 'startday', 'starttime'], \n",
    "                    right_on=['endstationid', 'stopday', 'stoptime'])\n",
    "df_demand_count = df_demand_count.drop(['endstationid', 'stopday', 'stoptime'], axis=1)\n",
    "df_demand_count = df_demand_count.rename(columns={'startstationid': 'id', 'startday': 'weekday','starttime': 'hour'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate netcount and demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_count['netcount'] = df_demand_count['stopcount'] - df_demand_count['startcount']\n",
    "df_demand_count = pd.merge(df_demand_count,df_demand_common_stations[['id','capacity']],on='id', how='left')\n",
    "\n",
    "#drop rows with stations that are not in capacity dataset\n",
    "df_demand_count = df_demand_count[np.isfinite(df_demand_count['capacity'])]\n",
    "\n",
    "#Demand function. Function is negated to give stations in higher demand a higher value\n",
    "df_demand_count['demand'] = -(df_demand_count['netcount'])/df_demand_count['capacity']\n",
    "\n",
    "#Convert id to int\n",
    "df_demand_count['id'] = df_demand_count['id'].apply(lambda x : int(x))\n",
    "\n",
    "#drop rows with stations that have 0 in capacity\n",
    "df_demand_count = df_demand_count[df_demand_count.capacity != 0]\n",
    "\n",
    "#drop duplicates\n",
    "df_demand_count = df_demand_count.drop_duplicates() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save filtered data to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_count.to_csv('capacity.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Business prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract hour of started trips, hour of ended trips and the day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df16.copy()\n",
    "list_of_starthours, list_of_stophours, list_of_weekdays = np.zeros(len(df_pred)), np.zeros(len(df_pred)), np.zeros(len(df_pred))\n",
    "\n",
    "for i in tqdm(range(len(df_pred))):\n",
    "    list_of_starthours[i] = df_pred.iloc[i].starttime.hour\n",
    "    list_of_stophours[i] = df_pred.iloc[i].stoptime.hour\n",
    "    list_of_weekdays[i] = df_pred.iloc[i].starttime.weekday()\n",
    "\n",
    "#Insert extra columns in dataset\n",
    "df_pred['starthour'] = list_of_starthours.astype(int)\n",
    "df_pred['endhour'] = list_of_stophours.astype(int)\n",
    "df_pred['weekday'] = list_of_weekdays.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dataframes with count for started and ended trips, sorted by weekday and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a dataframe with the count of started trips, sorted by station first, then weekday, then hour\n",
    "df1_pred = df_pred.groupby([df_pred['startstationid'], df_pred['weekday'], df_pred['starthour']])['startstationname'].count().reset_index(name = \"startcount\")\n",
    "df1_pred = df1_pred.rename(columns={'starthour':'hour','startstationid':'stationid'})\n",
    "# Makes a dataframe with the count of ended trips, sorted by station first, then weekday, then hour\n",
    "df2_pred = df_pred.groupby([df_pred['endstationid'], df_pred['weekday'], df_pred['endhour']])['endstationname'].count().reset_index(name='stopcount')\n",
    "df2_pred = df2_pred.rename(columns={'endhour':'hour','endstationid':'stationid'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataframes with start and stopcount. Add new column \"taken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_pred = df1_pred.merge(df2_pred, on = ['stationid', 'hour', 'weekday'])\n",
    "df3_pred['taken'] = df3_pred['stopcount'] - df3_pred['startcount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define input features and target values. Shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines input features and target values\n",
    "X = df3_pred[['stationid','weekday','hour']].values\n",
    "y1 = df3_pred[['taken']].values\n",
    "\n",
    "# Data Prep\n",
    "y = np.array([\"high demand\" if n<0 else \"low demand\" for n in y1])\n",
    "class_idx = {'high demand':1, 'low demand':0}\n",
    "\n",
    "y = np.array([class_idx[v] for v in y])\n",
    "y = np.eye(2)[y]\n",
    "\n",
    "\n",
    "# Shuffles the data\n",
    "random_idx = np.arange(X.shape[0])\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(random_idx)\n",
    "\n",
    "X = X[random_idx]\n",
    "y = y[random_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(min_samples_split=13, criterion = \"entropy\")\n",
    "DT.fit(X_train, y_train)\n",
    "predictions = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = y_test.argmax(axis=1)\n",
    "correct_preds = np.equal(true, predictions.argmax(axis=1))\n",
    "sum(correct_preds) / len(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X, y):\n",
    "        \n",
    "        # Predict\n",
    "        yhat = DT.predict(X)\n",
    "    \n",
    "        # Calculate MSE\n",
    "        return np.mean((y-yhat)**2) # * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create desicion tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(clf, features, labels, node_index=0):\n",
    "    \"\"\"Structure of rules in a fit decision tree classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : DecisionTreeClassifier\n",
    "        A tree that has already been fit.\n",
    "\n",
    "    features, labels : lists of str\n",
    "        The names of the features and labels, respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    node = {}\n",
    "    if clf.tree_.children_left[node_index] == -1:  # indicates leaf\n",
    "        count_labels = zip(clf.tree_.value[node_index, 0], labels)\n",
    "        node['name'] = ', '.join(('{} of {}'.format(int(count), label)\n",
    "                                  for count, label in count_labels))\n",
    "    else:\n",
    "        feature = features[clf.tree_.feature[node_index]]\n",
    "        threshold = clf.tree_.threshold[node_index]\n",
    "        node['name'] = '{} > {}'.format(feature, threshold)\n",
    "        left_index = clf.tree_.children_left[node_index]\n",
    "        right_index = clf.tree_.children_right[node_index]\n",
    "        node['children'] = [rules(clf, features, labels, right_index),\n",
    "                            rules(clf, features, labels, left_index)]\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create JSON file with decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['StationID', 'Weekday', 'Hour']\n",
    "target_names = [\"Unbusy\", \"Busy\"]\n",
    "\n",
    "r = rules(DT, feature_names, target_names)\n",
    "\n",
    "with open('rules6.json', 'w') as f:\n",
    "    f.write(json.dumps(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Genre. Which genre of data story did you use?\n",
    "\n",
    "Which tools did you use from each of the 3 categories of Visual Narrative (Figure 7 in Segal and Heer). Why?\n",
    "\n",
    "Which tools did you use from each of the 3 categories of Narrative Structure (Figure 7 in Segal and Heer). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualizations.\n",
    "\n",
    "Explain the visualizations you've chosen.\n",
    "\n",
    "Why are they right for the story you want to tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Discussion. Think critically about your creation\n",
    "What went well?,\n",
    "What is still missing? What could be improved?, Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
