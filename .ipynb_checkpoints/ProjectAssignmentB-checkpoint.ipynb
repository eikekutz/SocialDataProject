{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02806 - Social Data Analysis and Viszalization -  Project Assignment B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Package and data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import json\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataByYear(year):\n",
    "    '''\n",
    "    A function to combine the data records of a year to a dataframe.\n",
    "    Due to different headers in the individual files (spaces, upper and\n",
    "    lower case letters, etc.), these are normalized.\n",
    "    Output is a Pandas Dataframe for the requested year.\n",
    "    '''\n",
    "    name=\"\"\n",
    "    df=[]\n",
    "    \n",
    "    for k in range(1,13):\n",
    "        if k<10:\n",
    "            name = str(year)+\"0\"+str(k)\n",
    "        else:\n",
    "            name = str(year)+\"\"+str(k)\n",
    "        name +=\"-citibike-tripdata.csv\"\n",
    "        \n",
    "        df.append(pd.read_csv('data/'+name))\n",
    "        #set columns to lower case\n",
    "        df[k-1].columns = map(str.lower, df[k-1].columns)\n",
    "        #Filter data \n",
    "        df[k-1].columns = [x.replace(\" \",\"\") for x in list(df[k-1].columns)]\n",
    "        df[k-1][\"starttime\"] = pd.to_datetime(df[k-1][\"starttime\"], infer_datetime_format=True)\n",
    "        df[k-1][\"stoptime\"] = pd.to_datetime(df[k-1][\"stoptime\"], infer_datetime_format=True)\n",
    "        \n",
    "    return pd.concat(df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset of years 2016-2018\n",
    "bd16=getDataByYear(2016)\n",
    "bd17=getDataByYear(2017)\n",
    "bd18=getDataByYear(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all trip data\n",
    "frames = [df16,df17,df18]\n",
    "df_trips=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data from json url. Delete unnecessary columns\n",
    "with urllib.request.urlopen(\"https://feeds.citibikenyc.com/stations/stations.json\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    del data['executionTime']\n",
    "    data = data['stationBeanList']\n",
    "\n",
    "#Add data from json objects to list. Only include necessary fields\n",
    "stations = []\n",
    "for station in data:\n",
    "    stations.append([station['id'], station['stationName'], station['latitude'], station['longitude'], station['totalDocks']])\n",
    "\n",
    "#make dataframe of station info from list derived from json object\n",
    "df_stations = pd.DataFrame(stations, columns=[\"id\", \"stationname\", \"lat\", \"long\", \"capacity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Motivation.\n",
    "What is your dataset?\n",
    "\n",
    "Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "What was your goal for the end user's experience?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic stats. \n",
    "\n",
    "Let's understand the dataset better\n",
    "\n",
    "\n",
    "Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "\n",
    "Write a short section that discusses the dataset stats, containing key points/plots from your exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Analysis\n",
    "\n",
    "Describe your data analysis and explain what you've learned about the dataset.\n",
    "\n",
    "If relevant, talk about your machine-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete unnecessary columns and change to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand = df_trips.copy()\n",
    "del_cols = ['tripduration', 'startstationname', 'startstationlatitude', 'startstationlongitude', 'endstationname',\n",
    "       'endstationlatitude', 'endstationlongitude', 'usertype', 'birthyear', 'gender', 'bikeid']\n",
    "df_demand = df_demand.drop(del_cols, axis=1)\n",
    "df_demand['startday'] = df_demand['starttime'].dt.dayofweek\n",
    "df_demand['stopday'] = df_demand['stoptime'].dt.dayofweek\n",
    "df_demand['starttime'] = df_demand['starttime'].dt.hour\n",
    "df_demand['stoptime'] = df_demand['stoptime'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find stations that have both incoming and outgoing bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_demand = df_demand[['startstationid']].drop_duplicates()\n",
    "df3_demand = df_demand[['endstationid']].drop_duplicates()\n",
    "df4_demand = pd.merge(df2_demand,df3_demand, how='inner', left_on='startstationid', right_on='endstationid')\n",
    "df4_demand = df4_demand[['startstationid']].drop_duplicates().dropna()\n",
    "df4_demand['startstationid'] = df4_demand['startstationid'].apply(lambda x : int(x))\n",
    "df4_demand.columns = ['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find stations that are in both dataset, and merge these. Save data to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_common_stations = pd.merge(df1_stations, df4_demand, how='inner', on=['id'])\n",
    "df_demand_common_stations.to_csv('common_stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count bikes in and out for weekdays and hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_start = df_demand[['startstationid', 'startday','starttime']]\n",
    "df_demand_stop = df_demand[['endstationid', 'stopday', 'stoptime']]\n",
    "df_demand_start= pd.DataFrame({'startcount' : df_demand_start.groupby(['startstationid', 'startday', 'starttime']).size()}).reset_index()\n",
    "df_demand_stop = pd.DataFrame({'stopcount' : df_demand_stop.groupby(['endstationid', 'stopday', 'stoptime']).size()}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge in and out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_count = pd.merge(df_demand_start, df_demand_stop, left_on=['startstationid', 'startday', 'starttime'], \n",
    "                    right_on=['endstationid', 'stopday', 'stoptime'])\n",
    "df_demand_count = df_demand_count.drop(['endstationid', 'stopday', 'stoptime'], axis=1)\n",
    "df_demand_count = df_demand_count.rename(columns={'startstationid': 'id', 'startday': 'weekday','starttime': 'hour'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate netcount and demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_count['netcount'] = df_demand_count['stopcount'] - df_demand_count['startcount']\n",
    "df_demand_count = pd.merge(df_demand_count,df_demand_common_stations[['id','capacity']],on='id', how='left')\n",
    "\n",
    "#drop rows with stations that are not in capacity dataset\n",
    "df_demand_count = df_demand_count[np.isfinite(df_demand_count['capacity'])]\n",
    "\n",
    "#Demand function. Function is negated to give stations in higher demand a higher value\n",
    "df_demand_count['demand'] = -(df_demand_count['netcount'])/df_demand_count['capacity']\n",
    "\n",
    "#Convert id to int\n",
    "df_demand_count['id'] = df_demand_count['id'].apply(lambda x : int(x))\n",
    "\n",
    "#drop rows with stations that have 0 in capacity\n",
    "df_demand_count = df_demand_count[df_demand_count.capacity != 0]\n",
    "\n",
    "#drop duplicates\n",
    "df_demand_count = df_demand_count.drop_duplicates() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save filtered data to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_count.to_csv('capacity.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Business prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Genre. Which genre of data story did you use?\n",
    "\n",
    "Which tools did you use from each of the 3 categories of Visual Narrative (Figure 7 in Segal and Heer). Why?\n",
    "\n",
    "Which tools did you use from each of the 3 categories of Narrative Structure (Figure 7 in Segal and Heer). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualizations.\n",
    "\n",
    "Explain the visualizations you've chosen.\n",
    "\n",
    "Why are they right for the story you want to tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Discussion. Think critically about your creation\n",
    "What went well?,\n",
    "What is still missing? What could be improved?, Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
