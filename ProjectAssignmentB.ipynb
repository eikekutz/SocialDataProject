{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02806 - Social Data Analysis and Viszalization -  Project Assignment B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Package and data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import json\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data has been retreived from https://www.citibikenyc.com/system-data and https://feeds.citibikenyc.com/stations/stations.json. The data consists of features concerning bikeshare traffic in NYC, such as starttime, endtime, duration etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataByYear(year):\n",
    "    '''\n",
    "    A function to combine the data records of a year to a dataframe.\n",
    "    Due to different headers in the individual files (spaces, upper and\n",
    "    lower case letters, etc.), these are normalized.\n",
    "    Output is a Pandas Dataframe for the requested year.\n",
    "    '''\n",
    "    name=\"\"\n",
    "    df=[]\n",
    "    \n",
    "    for k in range(1,13):\n",
    "        if k<10:\n",
    "            name = str(year)+\"0\"+str(k)\n",
    "        else:\n",
    "            name = str(year)+\"\"+str(k)\n",
    "        name +=\"-citibike-tripdata.csv\"\n",
    "        \n",
    "        df.append(pd.read_csv('data/'+name))\n",
    "        #set columns to lower case\n",
    "        df[k-1].columns = map(str.lower, df[k-1].columns)\n",
    "        #Filter data \n",
    "        df[k-1].columns = [x.replace(\" \",\"\") for x in list(df[k-1].columns)]\n",
    "        df[k-1][\"starttime\"] = pd.to_datetime(df[k-1][\"starttime\"], infer_datetime_format=True)\n",
    "        df[k-1][\"stoptime\"] = pd.to_datetime(df[k-1][\"stoptime\"], infer_datetime_format=True)\n",
    "        \n",
    "    return pd.concat(df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dataset of years 2016-2018\n",
    "df16=getDataByYear(2016)\n",
    "df17=getDataByYear(2017)\n",
    "df18=getDataByYear(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge all trip data\n",
    "frames = [df16,df17,df18]\n",
    "df_trips=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tripduration</th>\n",
       "      <th>starttime</th>\n",
       "      <th>stoptime</th>\n",
       "      <th>startstationid</th>\n",
       "      <th>startstationname</th>\n",
       "      <th>startstationlatitude</th>\n",
       "      <th>startstationlongitude</th>\n",
       "      <th>endstationid</th>\n",
       "      <th>endstationname</th>\n",
       "      <th>endstationlatitude</th>\n",
       "      <th>endstationlongitude</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>usertype</th>\n",
       "      <th>birthyear</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>923</td>\n",
       "      <td>2016-01-01 00:00:41</td>\n",
       "      <td>2016-01-01 00:16:04</td>\n",
       "      <td>268.0</td>\n",
       "      <td>Howard St &amp; Centre St</td>\n",
       "      <td>40.719105</td>\n",
       "      <td>-73.999733</td>\n",
       "      <td>3002.0</td>\n",
       "      <td>South End Ave &amp; Liberty St</td>\n",
       "      <td>40.711512</td>\n",
       "      <td>-74.015756</td>\n",
       "      <td>22285</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>379</td>\n",
       "      <td>2016-01-01 00:00:45</td>\n",
       "      <td>2016-01-01 00:07:04</td>\n",
       "      <td>476.0</td>\n",
       "      <td>E 31 St &amp; 3 Ave</td>\n",
       "      <td>40.743943</td>\n",
       "      <td>-73.979661</td>\n",
       "      <td>498.0</td>\n",
       "      <td>Broadway &amp; W 32 St</td>\n",
       "      <td>40.748549</td>\n",
       "      <td>-73.988084</td>\n",
       "      <td>17827</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589</td>\n",
       "      <td>2016-01-01 00:00:48</td>\n",
       "      <td>2016-01-01 00:10:37</td>\n",
       "      <td>489.0</td>\n",
       "      <td>10 Ave &amp; W 28 St</td>\n",
       "      <td>40.750664</td>\n",
       "      <td>-74.001768</td>\n",
       "      <td>284.0</td>\n",
       "      <td>Greenwich Ave &amp; 8 Ave</td>\n",
       "      <td>40.739017</td>\n",
       "      <td>-74.002638</td>\n",
       "      <td>21997</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>889</td>\n",
       "      <td>2016-01-01 00:01:06</td>\n",
       "      <td>2016-01-01 00:15:56</td>\n",
       "      <td>268.0</td>\n",
       "      <td>Howard St &amp; Centre St</td>\n",
       "      <td>40.719105</td>\n",
       "      <td>-73.999733</td>\n",
       "      <td>3002.0</td>\n",
       "      <td>South End Ave &amp; Liberty St</td>\n",
       "      <td>40.711512</td>\n",
       "      <td>-74.015756</td>\n",
       "      <td>22794</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1480</td>\n",
       "      <td>2016-01-01 00:01:12</td>\n",
       "      <td>2016-01-01 00:25:52</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Central Park S &amp; 6 Ave</td>\n",
       "      <td>40.765909</td>\n",
       "      <td>-73.976342</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Central Park S &amp; 6 Ave</td>\n",
       "      <td>40.765909</td>\n",
       "      <td>-73.976342</td>\n",
       "      <td>14562</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tripduration           starttime            stoptime  startstationid  \\\n",
       "0           923 2016-01-01 00:00:41 2016-01-01 00:16:04           268.0   \n",
       "1           379 2016-01-01 00:00:45 2016-01-01 00:07:04           476.0   \n",
       "2           589 2016-01-01 00:00:48 2016-01-01 00:10:37           489.0   \n",
       "3           889 2016-01-01 00:01:06 2016-01-01 00:15:56           268.0   \n",
       "4          1480 2016-01-01 00:01:12 2016-01-01 00:25:52          2006.0   \n",
       "\n",
       "         startstationname  startstationlatitude  startstationlongitude  \\\n",
       "0   Howard St & Centre St             40.719105             -73.999733   \n",
       "1         E 31 St & 3 Ave             40.743943             -73.979661   \n",
       "2        10 Ave & W 28 St             40.750664             -74.001768   \n",
       "3   Howard St & Centre St             40.719105             -73.999733   \n",
       "4  Central Park S & 6 Ave             40.765909             -73.976342   \n",
       "\n",
       "   endstationid              endstationname  endstationlatitude  \\\n",
       "0        3002.0  South End Ave & Liberty St           40.711512   \n",
       "1         498.0          Broadway & W 32 St           40.748549   \n",
       "2         284.0       Greenwich Ave & 8 Ave           40.739017   \n",
       "3        3002.0  South End Ave & Liberty St           40.711512   \n",
       "4        2006.0      Central Park S & 6 Ave           40.765909   \n",
       "\n",
       "   endstationlongitude  bikeid    usertype  birthyear  gender  \n",
       "0           -74.015756   22285  Subscriber     1958.0       1  \n",
       "1           -73.988084   17827  Subscriber     1969.0       1  \n",
       "2           -74.002638   21997  Subscriber     1982.0       2  \n",
       "3           -74.015756   22794  Subscriber     1961.0       2  \n",
       "4           -73.976342   14562  Subscriber     1952.0       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import data from json url. Delete unnecessary columns\n",
    "with urllib.request.urlopen(\"https://feeds.citibikenyc.com/stations/stations.json\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    del data['executionTime']\n",
    "    data = data['stationBeanList']\n",
    "\n",
    "#Add data from json objects to list. Only include necessary fields\n",
    "stations = []\n",
    "for station in data:\n",
    "    stations.append([station['id'], station['stationName'], station['latitude'], station['longitude'], station['totalDocks']])\n",
    "\n",
    "#make dataframe of station info from list derived from json object\n",
    "df_stations = pd.DataFrame(stations, columns=[\"id\", \"stationname\", \"lat\", \"long\", \"capacity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>stationname</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72</td>\n",
       "      <td>W 52 St &amp; 11 Ave</td>\n",
       "      <td>40.767272</td>\n",
       "      <td>-73.993929</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>Franklin St &amp; W Broadway</td>\n",
       "      <td>40.719116</td>\n",
       "      <td>-74.006667</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>St James Pl &amp; Pearl St</td>\n",
       "      <td>40.711174</td>\n",
       "      <td>-74.000165</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83</td>\n",
       "      <td>Atlantic Ave &amp; Fort Greene Pl</td>\n",
       "      <td>40.683826</td>\n",
       "      <td>-73.976323</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119</td>\n",
       "      <td>Park Ave &amp; St Edwards St</td>\n",
       "      <td>40.696089</td>\n",
       "      <td>-73.978034</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                    stationname        lat       long  capacity\n",
       "0   72               W 52 St & 11 Ave  40.767272 -73.993929        55\n",
       "1   79       Franklin St & W Broadway  40.719116 -74.006667        33\n",
       "2   82         St James Pl & Pearl St  40.711174 -74.000165        27\n",
       "3   83  Atlantic Ave & Fort Greene Pl  40.683826 -73.976323        62\n",
       "4  119       Park Ave & St Edwards St  40.696089 -73.978034        19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Motivation.\n",
    "\n",
    "For this project we have worked with data concerning Bike Share traffic in New York City. With its many features, this dataset provides a great opportunity to analyze the infrastructure and use, as well as the efficiency, of the bike-share service. Members are often unable to find a bicycle, especially at peak times, and have to rely on other transport routes despite their paid membership. In the long run this could lead to a decrease in the number of members and thus stand in the way of a greener urban life.\n",
    "\n",
    "We therefore hope to be able to contribute to the maintenance of bike-sharing systems by analysing where bottlenecks exist and where there is a need to catch up in the infrastructure. We hope that this will be an analysis model that can also be applied to other cities and bike-sharing systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic stats. \n",
    "\n",
    "Let's understand the dataset better\n",
    "\n",
    "\n",
    "Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "\n",
    "Write a short section that discusses the dataset stats, containing key points/plots from your exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trip Data\n",
    "While the trip dataset is a big one, with nearly 48M rows (!) and 15 columns, it can be reduced a great deal by removing unnecessary columns. For the analyses only the columns with time and ID will be of interest. Station names and coordinates can later be derived from the other data set, *Station Data*. <br>\n",
    "For the exploratory analysis however, both station names and the coordinates are convenient to keep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2.1** Trip Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Station Data\n",
    "After removing unnecessary columns, the station data set consists of the station ID, the station name, the coordinates and the capacity of every station. The essential data of this dataset is the *capacity*. Keeping the station name and coordinates in this dataset, as opposed to in the trip dataset, is good for avoiding redundancy of data. When those columns are needed, they can easily be mapped to the trip data by the IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_stations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-772beb63a0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_stations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_stations' is not defined"
     ]
    }
   ],
   "source": [
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2.2** Station Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Analysis\n",
    "\n",
    "First: Getting a look of which stations are the most popular, and how many trips each station have. As this is only to get an estimate, only the from-trips are included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startStations=pd.DataFrame({'count' : df_trips.groupby('startstationname').startstationid.count()}).reset_index()\n",
    "startStations=startStations.sort_values(by=['count'],ascending=False)\n",
    "startStaTop=startStations[0:50]\n",
    "startStaBottom=startStations[-50:]\n",
    "#endStations=pd.DataFrame({'count' : df_trips.groupby('endstationname').endstationid.count()}).reset_index()\n",
    "#endStations=endStations.sort_values(by=['count'],ascending=False)\n",
    "#endSta=endStations[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the top and bottom 50 stations provides for an overview of the difference in activity of the stations in the data. As one can see in **Figure 2.3** below, the activity difference is quite substantial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the histogram with 50 bins\n",
    "f, ((ax1,ax2)) = plt.subplots(1,2,figsize=(30,15));\n",
    "my_cmap = ListedColormap(sns.color_palette('husl').as_hex())\n",
    "startStaTop.plot(kind='bar',xticks=startStaTop.index,rot=90,fontsize=9,figsize=(12,6),ax=ax1)\n",
    "ax1.set_xticklabels(startStaTop.startstationname);\n",
    "ax1.legend(['Top 50 Stations'])\n",
    "ax1.set_ylim([0,500000])\n",
    "\n",
    "startStaBottom.plot(kind='bar',xticks=startStaBottom.index,rot=90,fontsize=9,figsize=(12,6),ax=ax2)\n",
    "ax2.set_xticklabels(startStaBottom.startstationname);\n",
    "ax2.legend(['Bottom 50 Stations'])\n",
    "ax2.set_ylim([0,600])\n",
    "plt.savefig('demo.png', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2.3** Barchart of top and ottom 50 stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a big part of the analysis is going to deal with maps, a scatter plot of the stations is good to have. Already from the scatter plot in **Figure 2.4**, one can see where there are more stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stations in trip dataset\n",
    "m1 = folium.Map([40.730610, -73.935242], zoom_start=11)\n",
    "# mark each station as a point\n",
    "scatter_data = df_stations.drop_duplicates()\n",
    "for index, row in scatter_data.iterrows():\n",
    "    folium.CircleMarker([row['lat'], row['long']],\n",
    "                        radius=3,\n",
    "                        popup=str(row),\n",
    "                        fill_color=\"#3db7e4\", # divvy color\n",
    "                       ).add_to(m1)\n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2.4** Scatterplot of stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, the demand of stations will be analysed and predicted. In order to understand the data better for this analysis, the heatmap in **Figure 2.5** has been created. The heatmap, that plots the trips of every station, gives a good overview of the station activity - and hence an idea of the demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_stat=pd.DataFrame({'count' : df_trips.groupby(['startstationid','startstationlatitude','startstationlongitude']).startstationid.count()}).reset_index()\n",
    "start_stat = start_stat.rename(columns={'startstationid': 'id', 'startstationlatitude': 'lat','startstationlongitude': 'long'})\n",
    "\n",
    "end_stat=pd.DataFrame({'count' : df_trips.groupby(['endstationid','endstationlatitude','endstationlongitude']).startstationid.count()}).reset_index()\n",
    "end_stat = end_stat.rename(columns={'endstationid': 'id', 'endstationlatitude': 'lat','endstationlongitude': 'long'})\n",
    "\n",
    "start_end = pd.merge(start_stat, end_stat, left_on=['id','lat', 'long'], \n",
    "                     right_on=['id','lat', 'long'])\n",
    "start_end['count'] = start_end['count_x'] + start_end['count_y']\n",
    "start_end = start_end.drop(['count_x', 'count_y'], axis=1)\n",
    "start_end.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = folium.Map([40.730610, -73.935242], zoom_start=12)\n",
    "\n",
    "# convert to (n, 2) nd-array format for heatmap\n",
    "stationArr = start_end[['lat', 'long', 'count']].as_matrix()\n",
    "max_amount = float(start_end['count'].max())\n",
    "\n",
    "hm_wide = plugins.HeatMap(stationArr,\n",
    "                   min_opacity=0.2,\n",
    "                   max_val=max_amount,\n",
    "                   radius=15, blur=10, \n",
    "                   max_zoom=1)\n",
    "\n",
    "k.add_child(hm_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2.5** Heatmap of station activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Analysis\n",
    "\n",
    "Decision Tree: After the data-extraction and initial preparation, the data needs to be prepared for machine learning. The machine learning model that has been chosen, is a decision tree, which is a supervised machine learning method. Therefore the input data X and target values Y must be defined.\n",
    "The dataframe is therefore grouped by \"(station)ID\", \"weekday\", \"hour\", \"startcount\" (of bikes) and \"stopcount\". A final column \"netcount\" given by \"startcount\" - \"stopcount\", is then added to the dataframe. \n",
    "One observation/example of the input data X to the decision tree is then defined as [ID, \"weekday\", \"hour\"]. These input features are chosen, because it seems likely that there should be a correlation between these features and the business of a station. I.e. it seems likely that a station is more busy on a Tuesday at 8am, than on a Saturday at 12pm.\n",
    "The model is supposed to predict if the netcount is negative (\"High Demand\") or not (\"Low Demand\"). The target value y for one observation/example is therefore a one-hot encoded vector indicating \"High Demand\" or \"Low Demand\".\n",
    "The data is furthermore shuffled, before divided into training and test sets. The data is then fit (trained) on the decision tree, and tested on the test set. The accuracy is about 78.5% on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Demand Analysis and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete unnecessary columns and change to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demand = df_trips.copy()\n",
    "del_cols = ['tripduration', 'startstationname', 'startstationlatitude', 'startstationlongitude', 'endstationname',\n",
    "       'endstationlatitude', 'endstationlongitude', 'usertype', 'birthyear', 'gender', 'bikeid']\n",
    "df_demand = df_demand.drop(del_cols, axis=1)\n",
    "df_demand['startday'] = df_demand['starttime'].dt.dayofweek\n",
    "df_demand['stopday'] = df_demand['stoptime'].dt.dayofweek\n",
    "df_demand['starttime'] = df_demand['starttime'].dt.hour\n",
    "df_demand['stoptime'] = df_demand['stoptime'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count bikes in and out for weekdays and hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demand_start = df_demand[['startstationid', 'startday','starttime']]\n",
    "df_demand_stop = df_demand[['endstationid', 'stopday', 'stoptime']]\n",
    "df_demand_start= pd.DataFrame({'startcount' : df_demand_start.groupby(['startstationid', 'startday', 'starttime']).size()}).reset_index()\n",
    "df_demand_stop = pd.DataFrame({'stopcount' : df_demand_stop.groupby(['endstationid', 'stopday', 'stoptime']).size()}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge in and out data. Add netcount of bikes in each station for every hour and weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demand_count = pd.merge(df_demand_start, df_demand_stop, left_on=['startstationid', 'startday', 'starttime'], \n",
    "                    right_on=['endstationid', 'stopday', 'stoptime'])\n",
    "df_demand_count = df_demand_count.drop(['endstationid', 'stopday', 'stoptime'], axis=1)\n",
    "df_demand_count = df_demand_count.rename(columns={'startstationid': 'id', 'startday': 'weekday','starttime': 'hour'})\n",
    "df_demand_count['id'] = df_demand_count['id'].apply(lambda x : int(x))\n",
    "\n",
    "#Calculate netcount\n",
    "df_demand_count['netcount'] = df_demand_count['stopcount'] - df_demand_count['startcount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>startcount</th>\n",
       "      <th>stopcount</th>\n",
       "      <th>netcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>89</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>46</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "      <td>-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  weekday  hour  startcount  stopcount  netcount\n",
       "0  72        0     0          59         89        30\n",
       "1  72        0     1          54         46        -8\n",
       "2  72        0     2          33         23       -10\n",
       "3  72        0     3          20         12        -8\n",
       "4  72        0     4          47         18       -29"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demand_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Demand Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find stations that have both incoming and outgoing bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2_demand = df_demand[['startstationid']].drop_duplicates()\n",
    "df3_demand = df_demand[['endstationid']].drop_duplicates()\n",
    "df4_demand = pd.merge(df2_demand,df3_demand, how='inner', left_on='startstationid', right_on='endstationid')\n",
    "df4_demand = df4_demand[['startstationid']].drop_duplicates().dropna()\n",
    "df4_demand['startstationid'] = df4_demand['startstationid'].apply(lambda x : int(x))\n",
    "df4_demand.columns = ['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find stations that are in both dataset, and merge these. Save data to .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demand_common_stations = pd.merge(df_stations, df4_demand, how='inner', on=['id'])\n",
    "df_demand_common_stations.to_csv('common_stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demand_analysis = pd.merge(df_demand_count,df_demand_common_stations[['id','capacity']],on='id', how='left')\n",
    "\n",
    "#drop rows with stations that are not in capacity dataset\n",
    "df_demand_analysis = df_demand_analysis[np.isfinite(df_demand_analysis['capacity'])]\n",
    "\n",
    "#Demand function. Function is negated to give stations in higher demand a higher value\n",
    "df_demand_analysis['demand'] = -(df_demand_analysis['netcount'])/df_demand_analysis['capacity']\n",
    "\n",
    "#Convert id to int\n",
    "#df_demand_analysis['id'] = df_demand_analysis['id'].apply(lambda x : int(x))\n",
    "\n",
    "#drop rows with stations that have 0 in capacity\n",
    "df_demand_analysis = df_demand_analysis[df_demand_analysis.capacity != 0]\n",
    "\n",
    "#drop duplicates\n",
    "df_demand_analysis = df_demand_analysis.drop_duplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>startcount</th>\n",
       "      <th>stopcount</th>\n",
       "      <th>netcount</th>\n",
       "      <th>capacity</th>\n",
       "      <th>demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>89</td>\n",
       "      <td>30</td>\n",
       "      <td>55.0</td>\n",
       "      <td>-0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>46</td>\n",
       "      <td>-8</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.145455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>-10</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>-8</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.145455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "      <td>-29</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.527273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  weekday  hour  startcount  stopcount  netcount  capacity    demand\n",
       "0  72        0     0          59         89        30      55.0 -0.545455\n",
       "1  72        0     1          54         46        -8      55.0  0.145455\n",
       "2  72        0     2          33         23       -10      55.0  0.181818\n",
       "3  72        0     3          20         12        -8      55.0  0.145455\n",
       "4  72        0     4          47         18       -29      55.0  0.527273"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demand_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save filtered data to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_demand_analysis.to_csv('capacity2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Demand Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define input features and target values. Shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines input features and target values\n",
    "X = df_demand_count[['id','weekday','hour']].values\n",
    "y1 = df_demand_count[['netcount']].values\n",
    "\n",
    "# Data Prep\n",
    "y = np.array([\"high demand\" if n<0 else \"low demand\" for n in y1])\n",
    "class_idx = {'high demand':1, 'low demand':0}\n",
    "\n",
    "y = np.array([class_idx[v] for v in y])\n",
    "y = np.eye(2)[y]\n",
    "\n",
    "\n",
    "# Shuffles the data\n",
    "random_idx = np.arange(X.shape[0])\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(random_idx)\n",
    "\n",
    "X = X[random_idx]\n",
    "y = y[random_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For Jupyter Notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(min_samples_split=13, criterion = \"entropy\")\n",
    "DT.fit(X_train, y_train)\n",
    "predictions = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For Observable Notebook* <br>\n",
    "max_depth = 5 for the desicion tree to fit into the observable notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT_obs = DecisionTreeClassifier(min_samples_split=13, max_depth = 5, criterion = \"entropy\")\n",
    "DT_obs.fit(X_train, y_train)\n",
    "predictions_obs = DTobs.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For Jypyter Notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7855386479455472"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = y_test.argmax(axis=1)\n",
    "correct_preds = np.equal(true, predictions.argmax(axis=1))\n",
    "print(\"Accuracy:\", sum(correct_preds) / len(true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For Observable Notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6179038523217276"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = y_test.argmax(axis=1)\n",
    "correct_preds = np.equal(true, predictions_obs.argmax(axis=1))\n",
    "print(\"Accuracy:\", sum(correct_preds) / len(true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MSE(X, y, DT):\n",
    "        \n",
    "        # Predict\n",
    "        yhat = DT.predict(X)\n",
    "    \n",
    "        # Calculate MSE\n",
    "        return np.mean((y-yhat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For Jupyter Notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MSE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b65267317442>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generalization Error; \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MSE' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Generalization Error; \", MSE(X_test, y_test, DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For Observable Notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38222087594882576"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Generalization Error; \", MSE(X_test, y_test, DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create desicion tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rules(clf, features, labels, node_index=0):\n",
    "    \"\"\"Structure of rules in a fit decision tree classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : DecisionTreeClassifier\n",
    "        A tree that has already been fit.\n",
    "\n",
    "    features, labels : lists of str\n",
    "        The names of the features and labels, respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    node = {}\n",
    "    if clf.tree_.children_left[node_index] == -1:  # indicates leaf\n",
    "        count_labels = zip(clf.tree_.value[node_index, 0], labels)\n",
    "        node['name'] = ', '.join(('{} of {}'.format(int(count), label)\n",
    "                                  for count, label in count_labels))\n",
    "    else:\n",
    "        feature = features[clf.tree_.feature[node_index]]\n",
    "        threshold = clf.tree_.threshold[node_index]\n",
    "        node['name'] = '{} > {}'.format(feature, threshold)\n",
    "        left_index = clf.tree_.children_left[node_index]\n",
    "        right_index = clf.tree_.children_right[node_index]\n",
    "        node['children'] = [rules(clf, features, labels, right_index),\n",
    "                            rules(clf, features, labels, left_index)]\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create JSON file with decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = ['StationID', 'Weekday', 'Hour']\n",
    "target_names = [\"LD\", \"HD\"]\n",
    "\n",
    "r = rules(DT_obs, feature_names, target_names)\n",
    "\n",
    "with open('desicion_tree.json', 'w') as f:\n",
    "    f.write(json.dumps(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Genre. Which genre of data story did you use?\n",
    "\n",
    "The genres of visualization in this project is exclusively the narrated chart. This was chosen because of its simplistic style, and it provides a great opportunity for user interactivity, and user exploration. Due to the nature of the data-set (start-time, end-time etc) we felt this data-set called for visualizations using annotated charts.\n",
    "\n",
    "To aid the visual narrative, *timebars* were used for visual structuring (this is for example seen with the sliders for the heatmaps), zooming was used for highlighting, and animated transitions were used for user guidance.\n",
    "\n",
    "For structuring the narrative, we have the following tools: Hover highlighting in visualizations such as in the Chord Diagram, as well as a few buttons and sliders for navigation through the visualizations. Every visualization has a default view for the viewer to initially see. The ordering of the structure is user directed, as there is no one single way to go through the narrative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualizations.\n",
    "\n",
    "Explain the visualizations you've chosen.\n",
    "\n",
    "Why are they right for the story you want to tell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Discussion. Think critically about your creation\n",
    "What went well?,\n",
    "What is still missing? What could be improved?, Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"Where do people go and where are the hotspot of cycling related trafic?\"* - Main Contributor: Eike\n",
    "\n",
    "*\"Infrastructure. Which bike sharing stations are in higher demand throughout the week?\"* - Main Contributor: Lars\n",
    "\n",
    "*\"Machine Learning: Decision Tree\"* - Main Contributor: Holger\n",
    "\n",
    "*Merging, Writing, etc* - Everybody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://planspace.org/20151129-see_sklearn_trees_with_d3/  \n",
    "https://www.citibikenyc.com/system-data  \n",
    "https://feeds.citibikenyc.com/stations/stations.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
